{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f9ce97",
   "metadata": {},
   "source": [
    "## 2. Adam Optimizer and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da04257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.1, iteration count = 1000, theta = [           nan 1.74787125e-46]\n",
      "Alpha = 0.005, iteration count = 1000 theta = [9.33263619e-302 6.65396858e-003] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q4/9zg9sbjj6hxdsx18xy5j5ybw0000gn/T/ipykernel_44590/2637933898.py:11: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  gradient = np.array([100 * theta[0], theta[1]])\n",
      "/var/folders/q4/9zg9sbjj6hxdsx18xy5j5ybw0000gn/T/ipykernel_44590/2637933898.py:12: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta = theta - alpha * gradient\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Initial theta: [1, 1]\n",
    "# Max iteration: 1000\n",
    "iter_max = 1000\n",
    "\n",
    "# SGD without momentum\n",
    "def sgd(alpha):\n",
    "    theta = np.array([1.0, 1.0])\n",
    "\n",
    "    for i in range(iter_max):\n",
    "        gradient = np.array([100 * theta[0], theta[1]])\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "        if np.linalg.norm(gradient) < 0.001:\n",
    "            return i + 1, theta\n",
    "\n",
    "    return iter_max, theta\n",
    "\n",
    "iter_count1, theta1 = sgd(0.1)\n",
    "iter_count2, theta2 = sgd(0.005)\n",
    "\n",
    "print(f'Alpha = 0.1, iteration count = {iter_count1}, theta = {theta1}')\n",
    "print(f'Alpha = 0.005, iteration count = {iter_count2} theta = {theta2} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 0.1, momentum = 0.9 iteration count = 145, theta = [0.0004848  0.00052577]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# SGD with momentum\n",
    "def sgd_momentum(alpha, beta):\n",
    "    theta = np.array([1.0, 1.0])\n",
    "    m = np.array([0.0, 0.0])\n",
    "\n",
    "    for i in range(iter_max):\n",
    "        gradient = np.array([100 * theta[0], theta[1]])\n",
    "        # add the momentum in\n",
    "        m = beta * m + (1 - beta) * gradient\n",
    "        theta = theta - alpha * m\n",
    "\n",
    "        if np.linalg.norm(gradient) < 0.001:\n",
    "            return i + 1, theta\n",
    "    \n",
    "    return iter_max, theta \n",
    "\n",
    "iter_count3, theta3 = sgd_momentum(0.1, 0.9)\n",
    "print(f'Alpha = 0.1, momentum = 0.9 iteration count = {iter_count3}, theta = {theta3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc6109",
   "metadata": {},
   "source": [
    "### 2.1 Result finds\n",
    "Without momentum, for both alpha values, SGD wasn't able to converge to optimal solution.  \n",
    "Both hit the max iteration counts at 1000.  \n",
    "When alpha = 0.1, the learning rate is too large, and resulting a diverge making theta too big.\n",
    "When alpha = 0.005, theta slowing converging but after 1000 iterations theta2 is still larger than  \n",
    "the tolorance at 0.001\n",
    "\n",
    "With momentum, theta went below 0.001 at 145 runs. So the momentum makes alpha = 0.1 work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4d4af9",
   "metadata": {},
   "source": [
    "### 2.2 How momentum helps\n",
    "m is the rolling average of the gradient of the past gradients. It sounds like we will take the next step  \n",
    "based on the previous steps. \"Momentum\" sounds exactly like that. And this could prevent us from making  \n",
    "huge jumps and possibly diverge. Instead, we could maintain a fairly low speed and wont't change directions  \n",
    "while converging down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1a8da",
   "metadata": {},
   "source": [
    "### 2.3 Introducing v\n",
    "The parameters with smaller gradients get larger updates and vise versa.  Such as theta2 = 1 gets bigger  \n",
    "updates than theta1 = 100. This is due to the division. And larger value makes larger denominator making  \n",
    "slower progress hence. This could help on even out between large and small parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2e332",
   "metadata": {},
   "source": [
    "### 2.4 Dropout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
